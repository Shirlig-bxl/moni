"""
æ—¥å¿—è§£ææ¨¡å—
å®æ—¶è§£æè®­ç»ƒæ—¥å¿—ï¼Œæå–Lossã€Accuracyã€Throughputç­‰æŒ‡æ ‡
è¯†åˆ«å¼‚å¸¸äº‹ä»¶ (OOM, NaN, WARNING)
"""

import re
import time
import csv
import os
from datetime import datetime
from typing import Dict, Any, Optional, List, TextIO
import threading
import argparse
from pathlib import Path


class LogParser:
    """æ—¥å¿—è§£æå™¨ç±»"""
    
    def __init__(self, log_file: str, output_file: str = "training_metrics.csv", interval: int = 1):
        self.log_file = log_file
        self.output_file = output_file
        self.interval = interval
        self.running = False
        self.thread = None
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else ".", exist_ok=True)
        
        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        self._compile_patterns()
        
        # æ–‡ä»¶ä½ç½®è·Ÿè¸ª
        self.file_position = 0
        
        print(f"æ—¥å¿—è§£æå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ç›‘æ§æ—¥å¿—æ–‡ä»¶: {self.log_file}")
        print(f"è¾“å‡ºæ–‡ä»¶: {self.output_file}")
    
    def _compile_patterns(self):
        """ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼"""
        
        # è®­ç»ƒæŒ‡æ ‡æ¨¡å¼
        self.patterns = {
            # Stepä¿¡æ¯: Step 10 | Loss: 0.693147 | LR: 2.00e-05 | Throughput: 16.50 samples/s | Step Time: 0.970s
            'step_info': re.compile(
                r'Step\s+(\d+)\s*\|\s*Loss:\s*([\d\.\-e]+)\s*\|\s*LR:\s*([\d\.\-e]+)\s*\|\s*Throughput:\s*([\d\.\-e]+)\s*samples/s\s*\|\s*Step Time:\s*([\d\.\-e]+)s'
            ),
            
            # è¯„ä¼°ç»“æœ: eval_loss: 0.693147 | eval_accuracy: 0.500000
            'eval_metrics': re.compile(
                r'eval_(\w+):\s*([\d\.\-e]+)'
            ),
            
            # æ—¶é—´æˆ³: 2023-11-05 14:30:25
            'timestamp': re.compile(
                r'(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2})'
            ),
            
            # å¼‚å¸¸äº‹ä»¶æ¨¡å¼
            'nan_loss': re.compile(
                r'\[ANOMALY DETECTED\]\s*NaN/Inf Loss.*?step\s+(\d+).*?:\s*([\d\.\-e]*)',
                re.IGNORECASE
            ),
            
            'oom_error': re.compile(
                r'(CUDA.*out of memory|OutOfMemoryError|OOM)',
                re.IGNORECASE
            ),
            
            'high_loss': re.compile(
                r'\[ANOMALY DETECTED\]\s*High Loss.*?step\s+(\d+).*?:\s*([\d\.\-e]+)',
                re.IGNORECASE
            ),
            
            # è®­ç»ƒé˜¶æ®µè¯†åˆ«
            'train_begin': re.compile(r'è®­ç»ƒå¼€å§‹|Training.*begin', re.IGNORECASE),
            'train_end': re.compile(r'è®­ç»ƒç»“æŸ|Training.*end|Training.*complete', re.IGNORECASE),
            'eval_begin': re.compile(r'è¯„ä¼°.*å¼€å§‹|Evaluation.*begin', re.IGNORECASE),
            'eval_end': re.compile(r'è¯„ä¼°.*ç»“æŸ|Evaluation.*end', re.IGNORECASE),
            
            # é”™è¯¯å’Œè­¦å‘Š
            'error': re.compile(r'ERROR|Error|error', re.IGNORECASE),
            'warning': re.compile(r'WARNING|Warning|warning', re.IGNORECASE),
        }
    
    def parse_line(self, line: str, line_timestamp: Optional[str] = None) -> Dict[str, Any]:
        """è§£æå•è¡Œæ—¥å¿—"""
        result = {
            'timestamp': line_timestamp or datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'raw_line': line.strip()
        }
        
        # æå–æ—¶é—´æˆ³
        timestamp_match = self.patterns['timestamp'].search(line)
        if timestamp_match:
            result['timestamp'] = timestamp_match.group(1)
        
        # è§£æè®­ç»ƒæ­¥éª¤ä¿¡æ¯
        step_match = self.patterns['step_info'].search(line)
        if step_match:
            result.update({
                'step': int(step_match.group(1)),
                'loss': float(step_match.group(2)),
                'learning_rate': float(step_match.group(3)),
                'throughput': float(step_match.group(4)),
                'step_time': float(step_match.group(5)),
                'metric_type': 'training_step'
            })
        
        # è§£æè¯„ä¼°æŒ‡æ ‡
        eval_matches = self.patterns['eval_metrics'].findall(line)
        if eval_matches:
            result['metric_type'] = 'evaluation'
            for metric_name, metric_value in eval_matches:
                result[f'eval_{metric_name}'] = float(metric_value)
        
        # æ£€æµ‹å¼‚å¸¸äº‹ä»¶
        self._detect_anomalies(line, result)
        
        # æ£€æµ‹è®­ç»ƒé˜¶æ®µ
        self._detect_phases(line, result)
        
        return result
    
    def _detect_anomalies(self, line: str, result: Dict[str, Any]):
        """æ£€æµ‹å¼‚å¸¸äº‹ä»¶"""
        
        # NaN Losså¼‚å¸¸
        nan_match = self.patterns['nan_loss'].search(line)
        if nan_match:
            result.update({
                'event_nan_loss': 1,
                'anomaly_step': int(nan_match.group(1)) if nan_match.group(1) else None,
                'anomaly_value': nan_match.group(2) if nan_match.group(2) else 'NaN',
                'anomaly_type': 'nan_loss'
            })
        
        # OOMé”™è¯¯
        if self.patterns['oom_error'].search(line):
            result.update({
                'event_oom': 1,
                'anomaly_type': 'oom_error'
            })
        
        # é«˜Losså¼‚å¸¸
        high_loss_match = self.patterns['high_loss'].search(line)
        if high_loss_match:
            result.update({
                'event_high_loss': 1,
                'anomaly_step': int(high_loss_match.group(1)),
                'anomaly_value': float(high_loss_match.group(2)),
                'anomaly_type': 'high_loss'
            })
        
        # ä¸€èˆ¬é”™è¯¯å’Œè­¦å‘Š
        if self.patterns['error'].search(line):
            result['event_error'] = 1
        
        if self.patterns['warning'].search(line):
            result['event_warning'] = 1
    
    def _detect_phases(self, line: str, result: Dict[str, Any]):
        """æ£€æµ‹è®­ç»ƒé˜¶æ®µ"""
        
        if self.patterns['train_begin'].search(line):
            result['phase'] = 'train_begin'
        elif self.patterns['train_end'].search(line):
            result['phase'] = 'train_end'
        elif self.patterns['eval_begin'].search(line):
            result['phase'] = 'eval_begin'
        elif self.patterns['eval_end'].search(line):
            result['phase'] = 'eval_end'
    
    def read_new_lines(self) -> List[str]:
        """è¯»å–æ—¥å¿—æ–‡ä»¶ä¸­çš„æ–°è¡Œ"""
        new_lines = []
        
        try:
            if not os.path.exists(self.log_file):
                return new_lines
            
            with open(self.log_file, 'r', encoding='utf-8', errors='ignore') as f:
                # ç§»åŠ¨åˆ°ä¸Šæ¬¡è¯»å–çš„ä½ç½®
                f.seek(self.file_position)
                
                # è¯»å–æ–°è¡Œ
                new_lines = f.readlines()
                
                # æ›´æ–°æ–‡ä»¶ä½ç½®
                self.file_position = f.tell()
                
        except Exception as e:
            print(f"è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
        
        return new_lines
    
    def write_csv_header(self):
        """å†™å…¥CSVæ–‡ä»¶å¤´"""
        fieldnames = [
            'timestamp', 'step', 'loss', 'learning_rate', 'throughput', 'step_time',
            'eval_loss', 'eval_accuracy', 'eval_f1', 'eval_precision', 'eval_recall',
            'metric_type', 'phase',
            'event_nan_loss', 'event_oom', 'event_high_loss', 'event_error', 'event_warning',
            'anomaly_type', 'anomaly_step', 'anomaly_value',
            'raw_line'
        ]
        
        with open(self.output_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
    
    def monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        print(f"å¼€å§‹æ—¥å¿—ç›‘æ§ï¼Œè¾“å‡ºæ–‡ä»¶: {self.output_file}")
        print(f"ç›‘æ§é—´éš”: {self.interval}ç§’")
        
        # å†™å…¥CSVå¤´
        self.write_csv_header()
        
        while self.running:
            try:
                new_lines = self.read_new_lines()
                
                if new_lines:
                    parsed_data = []
                    
                    for line in new_lines:
                        if line.strip():  # è·³è¿‡ç©ºè¡Œ
                            parsed_result = self.parse_line(line)
                            parsed_data.append(parsed_result)
                    
                    # å†™å…¥è§£æç»“æœ
                    if parsed_data:
                        self._write_parsed_data(parsed_data)
                        
                        # æ‰“å°é‡è¦äº‹ä»¶
                        for data in parsed_data:
                            self._print_important_events(data)
                
                time.sleep(self.interval)
                
            except Exception as e:
                print(f"ç›‘æ§å¾ªç¯é”™è¯¯: {e}")
                time.sleep(self.interval)
    
    def _write_parsed_data(self, parsed_data: List[Dict[str, Any]]):
        """å†™å…¥è§£æåçš„æ•°æ®"""
        fieldnames = [
            'timestamp', 'step', 'loss', 'learning_rate', 'throughput', 'step_time',
            'eval_loss', 'eval_accuracy', 'eval_f1', 'eval_precision', 'eval_recall',
            'metric_type', 'phase',
            'event_nan_loss', 'event_oom', 'event_high_loss', 'event_error', 'event_warning',
            'anomaly_type', 'anomaly_step', 'anomaly_value',
            'raw_line'
        ]
        
        with open(self.output_file, 'a', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            
            for data in parsed_data:
                # å¡«å……ç¼ºå¤±å­—æ®µ
                row = {field: data.get(field, '') for field in fieldnames}
                writer.writerow(row)
    
    def _print_important_events(self, data: Dict[str, Any]):
        """æ‰“å°é‡è¦äº‹ä»¶"""
        timestamp = data.get('timestamp', '')
        
        # è®­ç»ƒæŒ‡æ ‡
        if data.get('metric_type') == 'training_step':
            step = data.get('step', 0)
            loss = data.get('loss', 0)
            throughput = data.get('throughput', 0)
            print(f"[{timestamp}] Step {step}: Loss={loss:.6f}, Throughput={throughput:.2f} samples/s")
        
        # è¯„ä¼°æŒ‡æ ‡
        elif data.get('metric_type') == 'evaluation':
            eval_metrics = {k: v for k, v in data.items() if k.startswith('eval_') and v != ''}
            if eval_metrics:
                metrics_str = ', '.join([f"{k}={v:.6f}" for k, v in eval_metrics.items()])
                print(f"[{timestamp}] Evaluation: {metrics_str}")
        
        # å¼‚å¸¸äº‹ä»¶
        if data.get('event_nan_loss'):
            print(f"[{timestamp}] âš ï¸  ANOMALY: NaN Loss detected at step {data.get('anomaly_step', 'unknown')}")
        
        if data.get('event_oom'):
            print(f"[{timestamp}] âš ï¸  ANOMALY: Out of Memory Error detected")
        
        if data.get('event_high_loss'):
            print(f"[{timestamp}] âš ï¸  ANOMALY: High Loss detected at step {data.get('anomaly_step', 'unknown')}: {data.get('anomaly_value', 'unknown')}")
        
        # é˜¶æ®µå˜åŒ–
        if data.get('phase'):
            phase = data.get('phase')
            print(f"[{timestamp}] ğŸ“ Phase: {phase}")
    
    def start(self):
        """å¯åŠ¨ç›‘æ§"""
        if self.running:
            print("ç›‘æ§å·²åœ¨è¿è¡Œä¸­")
            return
        
        self.running = True
        self.thread = threading.Thread(target=self.monitor_loop)
        self.thread.daemon = True
        self.thread.start()
        print("æ—¥å¿—ç›‘æ§å·²å¯åŠ¨")
    
    def stop(self):
        """åœæ­¢ç›‘æ§"""
        if not self.running:
            print("ç›‘æ§æœªåœ¨è¿è¡Œ")
            return
        
        self.running = False
        if self.thread:
            self.thread.join(timeout=5)
        print("æ—¥å¿—ç›‘æ§å·²åœæ­¢")
    
    def __enter__(self):
        self.start()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.stop()


def parse_log_file(log_file: str, output_file: str = None) -> List[Dict[str, Any]]:
    """è§£ææ•´ä¸ªæ—¥å¿—æ–‡ä»¶ï¼ˆéå®æ—¶ï¼‰"""
    
    if output_file is None:
        output_file = log_file.replace('.log', '_parsed.csv')
    
    parser = LogParser(log_file, output_file)
    
    print(f"è§£ææ—¥å¿—æ–‡ä»¶: {log_file}")
    
    try:
        with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
            lines = f.readlines()
        
        parsed_data = []
        for line in lines:
            if line.strip():
                result = parser.parse_line(line)
                parsed_data.append(result)
        
        # å†™å…¥ç»“æœ
        parser.write_csv_header()
        parser._write_parsed_data(parsed_data)
        
        print(f"è§£æå®Œæˆï¼Œå…±å¤„ç† {len(parsed_data)} è¡Œ")
        print(f"ç»“æœä¿å­˜åˆ°: {output_file}")
        
        return parsed_data
        
    except Exception as e:
        print(f"è§£ææ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
        return []


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ—¥å¿—è§£æå·¥å…·")
    parser.add_argument("log_file", help="è¦ç›‘æ§çš„æ—¥å¿—æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", "-o", help="è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--interval", "-i", type=int, default=1, help="ç›‘æ§é—´éš”ï¼ˆç§’ï¼‰")
    parser.add_argument("--parse-only", action="store_true", help="ä»…è§£æç°æœ‰æ–‡ä»¶ï¼Œä¸è¿›è¡Œå®æ—¶ç›‘æ§")
    
    args = parser.parse_args()
    
    if args.parse_only:
        # ä»…è§£æç°æœ‰æ–‡ä»¶
        parse_log_file(args.log_file, args.output)
    else:
        # å®æ—¶ç›‘æ§
        output_file = args.output or "training_metrics.csv"
        log_parser = LogParser(args.log_file, output_file, args.interval)
        
        try:
            log_parser.start()
            print("æŒ‰ Ctrl+C åœæ­¢ç›‘æ§")
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            print("\nç”¨æˆ·ä¸­æ–­")
        finally:
            log_parser.stop()


if __name__ == "__main__":
    main()