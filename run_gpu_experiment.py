#!/usr/bin/env python3
"""
GPUç¯å¢ƒå®Œæ•´å®éªŒè„šæœ¬
ä¸“ä¸ºGoogle Colab GPUç¯å¢ƒè®¾è®¡
"""

import os
import sys
import time
import threading
import csv
import json
from datetime import datetime
from pathlib import Path
import subprocess


def check_gpu_environment():
    """æ£€æŸ¥GPUç¯å¢ƒ"""
    print("ğŸ” æ£€æŸ¥GPUç¯å¢ƒ...")
    
    try:
        import torch
        print(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"âœ… CUDAå¯ç”¨: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"âœ… GPUæ•°é‡: {torch.cuda.device_count()}")
            print(f"âœ… GPUåç§°: {torch.cuda.get_device_name(0)}")
            print(f"âœ… GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
            return True
        else:
            print("âŒ CUDAä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥Colabè¿è¡Œæ—¶è®¾ç½®")
            print("ğŸ’¡ è§£å†³æ–¹æ¡ˆ: è¿è¡Œæ—¶ â†’ æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ â†’ GPU")
            return False
            
    except ImportError:
        print("âŒ PyTorchæœªå®‰è£…")
        return False


def install_dependencies():
    """å®‰è£…å¿…è¦çš„ä¾èµ–"""
    print("ğŸ“¦ å®‰è£…ä¾èµ–åŒ…...")
    
    packages = [
        "torch torchvision torchaudio",
        "transformers datasets accelerate evaluate",
        "pandas numpy scipy scikit-learn",
        "psutil nvidia-ml-py3",
        "pyyaml loguru tqdm"
    ]
    
    for package in packages:
        print(f"å®‰è£…: {package}")
        result = subprocess.run(f"pip install {package}", shell=True, capture_output=True, text=True)
        if result.returncode != 0:
            print(f"âš ï¸ å®‰è£…å¤±è´¥: {package}")
        else:
            print(f"âœ… å®‰è£…æˆåŠŸ: {package}")


class GPUSystemMonitor:
    """GPUå’Œç³»ç»Ÿç›‘æ§å™¨"""
    
    def __init__(self, output_file, interval=1):
        self.output_file = output_file
        self.interval = interval
        self.running = False
        
    def start(self):
        """å¼€å§‹ç›‘æ§"""
        import psutil
        
        # å°è¯•å¯¼å…¥GPUç›‘æ§
        try:
            import nvidia_ml_py as nvml
            nvml.nvmlInit()
            gpu_available = True
            handle = nvml.nvmlDeviceGetHandleByIndex(0)
        except:
            try:
                import pynvml as nvml
                nvml.nvmlInit()
                gpu_available = True
                handle = nvml.nvmlDeviceGetHandleByIndex(0)
            except:
                gpu_available = False
                print("âš ï¸ GPUç›‘æ§ä¸å¯ç”¨ï¼Œä»…ç›‘æ§ç³»ç»ŸæŒ‡æ ‡")
        
        self.running = True
        
        # åˆ›å»ºCSVæ–‡ä»¶
        headers = ['timestamp', 'cpu_percent', 'memory_percent', 'disk_percent']
        if gpu_available:
            headers.extend(['gpu_util', 'gpu_memory_used', 'gpu_memory_total', 'gpu_temperature'])
        
        with open(self.output_file, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(headers)
        
        print(f"âœ… ç›‘æ§å·²å¯åŠ¨: {self.output_file}")
        
        while self.running:
            try:
                timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                
                # ç³»ç»ŸæŒ‡æ ‡
                cpu_percent = psutil.cpu_percent(interval=0.1)
                memory_percent = psutil.virtual_memory().percent
                disk_percent = psutil.disk_usage('/').percent
                
                row = [timestamp, cpu_percent, memory_percent, disk_percent]
                
                # GPUæŒ‡æ ‡
                if gpu_available:
                    try:
                        # GPUåˆ©ç”¨ç‡
                        util = nvml.nvmlDeviceGetUtilizationRates(handle)
                        gpu_util = util.gpu
                        
                        # GPUå†…å­˜
                        mem_info = nvml.nvmlDeviceGetMemoryInfo(handle)
                        gpu_memory_used = mem_info.used // 1024 // 1024  # MB
                        gpu_memory_total = mem_info.total // 1024 // 1024  # MB
                        
                        # GPUæ¸©åº¦
                        gpu_temp = nvml.nvmlDeviceGetTemperature(handle, nvml.NVML_TEMPERATURE_GPU)
                        
                        row.extend([gpu_util, gpu_memory_used, gpu_memory_total, gpu_temp])
                        
                        print(f"[{timestamp}] GPU0: Util={gpu_util}%, Mem={gpu_memory_used}/{gpu_memory_total}MB ({gpu_memory_used/gpu_memory_total*100:.1f}%), Temp={gpu_temp}Â°C")
                        print(f"[{timestamp}] CPU: {cpu_percent:.1f}%, Memory: {memory_percent:.1f}%, Disk: {disk_percent:.1f}%")
                        
                    except Exception as e:
                        print(f"GPUç›‘æ§é”™è¯¯: {e}")
                        row.extend([0, 0, 0, 0])
                else:
                    print(f"[{timestamp}] CPU: {cpu_percent:.1f}%, Memory: {memory_percent:.1f}%, Disk: {disk_percent:.1f}%")
                
                # å†™å…¥CSV
                with open(self.output_file, 'a', newline='') as f:
                    writer = csv.writer(f)
                    writer.writerow(row)
                
                time.sleep(self.interval)
                
            except Exception as e:
                print(f"ç›‘æ§é”™è¯¯: {e}")
                break
    
    def stop(self):
        """åœæ­¢ç›‘æ§"""
        self.running = False
        print("âœ… ç›‘æ§å·²åœæ­¢")


def run_real_bert_training():
    """è¿è¡ŒçœŸå®çš„BERTè®­ç»ƒï¼ˆGPUç‰ˆæœ¬ï¼‰"""
    print("ğŸš€ å¼€å§‹çœŸå®çš„BERT-IMDBè®­ç»ƒï¼ˆGPUç‰ˆæœ¬ï¼‰...")
    
    training_script = """
import torch
import torch.nn as nn
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    TrainingArguments, Trainer, DataCollatorWithPadding
)
from datasets import load_dataset
import numpy as np
from datetime import datetime
import sys

def main():
    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - INFO - å¼€å§‹BERT-IMDBå¾®è°ƒè®­ç»ƒ")
    
    # æ£€æŸ¥GPU
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - INFO - ä½¿ç”¨è®¾å¤‡: {device}")
    
    if torch.cuda.is_available():
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - INFO - GPU: {torch.cuda.get_device_name(0)}")
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - INFO - GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    model_name = "distilbert-base-uncased"
    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - INFO - åŠ è½½æ¨¡å‹: {model_name}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)
    model.to(device)
    
    # åŠ è½½æ•°æ®é›†
    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - INFO - åŠ è½½IMDBæ•°æ®é›†...")
    dataset = load_dataset("imdb")
    
    # é™åˆ¶æ•°æ®é‡ä»¥åŠ å¿«è®­ç»ƒ
    train_dataset = dataset["train"].select(range(1000))  # åªç”¨1000ä¸ªæ ·æœ¬
    eval_dataset = dataset["test"].select(range(200))     # åªç”¨200ä¸ªæ ·æœ¬
    
    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - INFO - è®­ç»ƒæ ·æœ¬: {len(train_dataset)}, éªŒè¯æ ·æœ¬: {len(eval_dataset)}")
    
    # æ•°æ®é¢„å¤„ç†
    def preprocess_function(examples):
        return tokenizer(examples["text"], truncation=True, padding=True, max_length=256)
    
    train_dataset = train_dataset.map(preprocess_function, batched=True)
    eval_dataset = eval_dataset.map(preprocess_function, batched=True)
    
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir="./gpu_results",
        num_train_epochs=2,
        per_device_train_batch_size=16,  # ä½¿ç”¨è¾ƒå¤§çš„batch sizeæ¥å……åˆ†åˆ©ç”¨GPU
        per_device_eval_batch_size=16,
        warmup_steps=50,
        weight_decay=0.01,
        logging_dir="./gpu_logs",
        logging_steps=10,
        eval_steps=50,
        save_steps=50,
        evaluation_strategy="steps",
        load_best_model_at_end=True,
        metric_for_best_model="accuracy",
        greater_is_better=True,
        report_to=None,  # ç¦ç”¨wandbç­‰
        dataloader_pin_memory=True,
        fp16=torch.cuda.is_available(),  # å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
    )
    
    # æ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    
    # è¯„ä¼°å‡½æ•°
    def compute_metrics(eval_pred):
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=1)
        accuracy = (predictions == labels).mean()
        return {"accuracy": accuracy}
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )
    
    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - INFO - å¼€å§‹è®­ç»ƒ...")
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()
    
    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - INFO - è®­ç»ƒå®Œæˆï¼")
    
    # æœ€ç»ˆè¯„ä¼°
    eval_results = trainer.evaluate()
    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - INFO - æœ€ç»ˆè¯„ä¼°ç»“æœ: {eval_results}")

if __name__ == "__main__":
    main()
"""
    
    # å†™å…¥è®­ç»ƒè„šæœ¬
    with open("gpu_bert_training.py", "w") as f:
        f.write(training_script)
    
    # è¿è¡Œè®­ç»ƒè„šæœ¬å¹¶æ•è·è¾“å‡º
    print("æ‰§è¡ŒGPUè®­ç»ƒè„šæœ¬...")
    process = subprocess.Popen(
        ["python", "gpu_bert_training.py"],
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        universal_newlines=True,
        bufsize=1
    )
    
    # å®æ—¶è¾“å‡ºå¹¶ä¿å­˜æ—¥å¿—
    log_file = "gpu_training.log"
    with open(log_file, "w") as f:
        for line in process.stdout:
            print(line.strip())
            f.write(line)
            f.flush()
    
    process.wait()
    print(f"âœ… GPUè®­ç»ƒå®Œæˆï¼Œæ—¥å¿—ä¿å­˜åˆ°: {log_file}")
    return log_file


def run_gpu_experiment():
    """è¿è¡Œå®Œæ•´çš„GPUå®éªŒ"""
    print("ğŸš€ GPUç¯å¢ƒå®Œæ•´å®éªŒ")
    print("=" * 50)
    
    # 1. æ£€æŸ¥GPUç¯å¢ƒ
    if not check_gpu_environment():
        print("âŒ GPUç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œé€€å‡ºå®éªŒ")
        return
    
    # 2. å®‰è£…ä¾èµ–ï¼ˆåœ¨Colabä¸­å¯èƒ½éœ€è¦ï¼‰
    try:
        import transformers
        import datasets
        print("âœ… ä¾èµ–åŒ…å·²å®‰è£…")
    except ImportError:
        install_dependencies()
    
    # 3. å¯åŠ¨ç›‘æ§
    print("\nğŸ“Š å¯åŠ¨GPUå’Œç³»ç»Ÿç›‘æ§...")
    monitor = GPUSystemMonitor("gpu_system_metrics.csv", interval=2)
    monitor_thread = threading.Thread(target=monitor.start)
    monitor_thread.daemon = True
    monitor_thread.start()
    
    time.sleep(3)  # ç­‰å¾…ç›‘æ§å¯åŠ¨
    
    # 4. è¿è¡ŒçœŸå®çš„BERTè®­ç»ƒ
    print("\nğŸš€ å¼€å§‹çœŸå®çš„BERTè®­ç»ƒ...")
    log_file = run_real_bert_training()
    
    # 5. ç­‰å¾…ä¸€æ®µæ—¶é—´æ”¶é›†æ›´å¤šç›‘æ§æ•°æ®
    print("\nâ³ ç­‰å¾…ç›‘æ§æ•°æ®æ”¶é›†...")
    time.sleep(10)
    
    # 6. åœæ­¢ç›‘æ§
    monitor.stop()
    time.sleep(2)
    
    # 7. åˆ†æç»“æœ
    print("\nğŸ“Š å®éªŒç»“æœåˆ†æ...")
    
    files = [
        "gpu_training.log",
        "gpu_system_metrics.csv",
        "gpu_bert_training.py"
    ]
    
    total_size = 0
    for file in files:
        if os.path.exists(file):
            size = os.path.getsize(file)
            total_size += size
            print(f"  âœ… {file} ({size:,} bytes)")
        else:
            print(f"  âŒ {file} (æœªç”Ÿæˆ)")
    
    print(f"\nğŸ“Š æ€»æ•°æ®é‡: {total_size:,} bytes ({total_size/1024/1024:.2f} MB)")
    
    # 8. æ£€æŸ¥GPUä½¿ç”¨æƒ…å†µ
    if os.path.exists("gpu_system_metrics.csv"):
        try:
            import pandas as pd
            df = pd.read_csv("gpu_system_metrics.csv")
            
            if 'gpu_util' in df.columns:
                max_gpu_util = df['gpu_util'].max()
                avg_gpu_util = df['gpu_util'].mean()
                max_gpu_memory = df['gpu_memory_used'].max()
                
                print(f"\nğŸ¯ GPUä½¿ç”¨æƒ…å†µåˆ†æ:")
                print(f"  - æœ€å¤§GPUåˆ©ç”¨ç‡: {max_gpu_util:.1f}%")
                print(f"  - å¹³å‡GPUåˆ©ç”¨ç‡: {avg_gpu_util:.1f}%")
                print(f"  - æœ€å¤§GPUå†…å­˜ä½¿ç”¨: {max_gpu_memory:.0f} MB")
                
                if max_gpu_util > 50:
                    print("  âœ… GPUè¢«å……åˆ†åˆ©ç”¨ï¼")
                else:
                    print("  âš ï¸ GPUåˆ©ç”¨ç‡è¾ƒä½ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´batch size")
            else:
                print("  âŒ æœªæ£€æµ‹åˆ°GPUä½¿ç”¨æ•°æ®")
                
        except Exception as e:
            print(f"  âŒ åˆ†æGPUä½¿ç”¨æƒ…å†µå¤±è´¥: {e}")
    
    print("\nğŸ‰ GPUå®éªŒå®Œæˆï¼")
    
    if os.path.exists("gpu_system_metrics.csv"):
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
        print("1. åˆ†ægpu_system_metrics.csvä¸­çš„GPUä½¿ç”¨æ¨¡å¼")
        print("2. åŸºäºçœŸå®GPUè®­ç»ƒæ•°æ®è¿›è¡Œå¼‚å¸¸æ£€æµ‹ç ”ç©¶")
        print("3. å¯¹æ¯”CPUå’ŒGPUç¯å¢ƒä¸‹çš„ç³»ç»Ÿè¡Œä¸ºå·®å¼‚")


def main():
    """ä¸»å‡½æ•°"""
    try:
        run_gpu_experiment()
    except KeyboardInterrupt:
        print("\nâ¹ï¸ å®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å®éªŒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()