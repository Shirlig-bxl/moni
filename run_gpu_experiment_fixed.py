#!/usr/bin/env python3
"""
ä¿®å¤ç‰ˆGPUå®éªŒè„šæœ¬ - ç¡®ä¿GPUå¾—åˆ°å……åˆ†åˆ©ç”¨
ä¸“é—¨è§£å†³GPUåˆ©ç”¨ç‡ä¸º0%çš„é—®é¢˜
"""

import os
import sys
import time
import threading
import subprocess
from datetime import datetime
from pathlib import Path

def check_and_fix_gpu_environment():
    """æ£€æŸ¥å¹¶ä¿®å¤GPUç¯å¢ƒ"""
    print("ğŸ”§ æ£€æŸ¥å¹¶ä¿®å¤GPUç¯å¢ƒ...")
    
    try:
        import torch
        print(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
        
        if not torch.cuda.is_available():
            print("âŒ CUDAä¸å¯ç”¨ï¼")
            print("ğŸ’¡ è¯·åœ¨Google Colabä¸­:")
            print("   1. è¿è¡Œæ—¶ â†’ æ›´æ”¹è¿è¡Œæ—¶ç±»å‹")
            print("   2. ç¡¬ä»¶åŠ é€Ÿå™¨ â†’ GPU")
            print("   3. ä¿å­˜å¹¶é‡æ–°è¿æ¥")
            return False
        
        print(f"âœ… CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"âœ… GPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"âœ… GPUåç§°: {torch.cuda.get_device_name(0)}")
        
        # æµ‹è¯•GPUåŸºæœ¬åŠŸèƒ½
        device = torch.device('cuda')
        test_tensor = torch.randn(100, 100, device=device)
        result = torch.matmul(test_tensor, test_tensor)
        print(f"âœ… GPUåŸºç¡€è®¡ç®—æµ‹è¯•é€šè¿‡")
        
        return True
        
    except ImportError:
        print("âŒ PyTorchæœªå®‰è£…")
        return False
    except Exception as e:
        print(f"âŒ GPUæµ‹è¯•å¤±è´¥: {e}")
        return False

def create_intensive_gpu_training_script():
    """åˆ›å»ºGPUå¯†é›†å‹è®­ç»ƒè„šæœ¬"""
    
    training_script = '''
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import time
import numpy as np
from datetime import datetime

def log_with_timestamp(message):
    """å¸¦æ—¶é—´æˆ³çš„æ—¥å¿—"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print(f"[{timestamp}] {message}")

def create_large_model():
    """åˆ›å»ºä¸€ä¸ªè¾ƒå¤§çš„æ¨¡å‹ä»¥å……åˆ†åˆ©ç”¨GPU"""
    model = nn.Sequential(
        nn.Linear(2048, 1024),
        nn.ReLU(),
        nn.Dropout(0.2),
        nn.Linear(1024, 512),
        nn.ReLU(),
        nn.Dropout(0.2),
        nn.Linear(512, 256),
        nn.ReLU(),
        nn.Linear(256, 128),
        nn.ReLU(),
        nn.Linear(128, 10)  # 10åˆ†ç±»
    )
    return model

def main():
    log_with_timestamp("å¼€å§‹GPUå¯†é›†å‹è®­ç»ƒ")
    
    # å¼ºåˆ¶ä½¿ç”¨GPU
    if not torch.cuda.is_available():
        log_with_timestamp("ERROR: CUDAä¸å¯ç”¨ï¼Œæ— æ³•ç»§ç»­")
        return
    
    device = torch.device('cuda')
    log_with_timestamp(f"ä½¿ç”¨è®¾å¤‡: {device}")
    log_with_timestamp(f"GPUåç§°: {torch.cuda.get_device_name(0)}")
    
    # åˆ›å»ºå¤§æ¨¡å‹
    log_with_timestamp("åˆ›å»ºå¤§å‹ç¥ç»ç½‘ç»œ...")
    model = create_large_model().to(device)
    
    # è®¡ç®—æ¨¡å‹å‚æ•°
    total_params = sum(p.numel() for p in model.parameters())
    log_with_timestamp(f"æ¨¡å‹å‚æ•°æ€»æ•°: {total_params:,}")
    
    # éªŒè¯æ¨¡å‹åœ¨GPUä¸Š
    model_device = next(model.parameters()).device
    log_with_timestamp(f"æ¨¡å‹è®¾å¤‡: {model_device}")
    
    # åˆ›å»ºå¤§æ‰¹é‡æ•°æ®
    batch_size = 256  # å¢å¤§batch size
    input_size = 2048
    num_batches = 100
    
    log_with_timestamp(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
    log_with_timestamp(f"è¾“å…¥ç»´åº¦: {input_size}")
    log_with_timestamp(f"æ€»æ‰¹æ¬¡æ•°: {num_batches}")
    
    # åˆ›å»ºä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    
    log_with_timestamp("å¼€å§‹è®­ç»ƒå¾ªç¯...")
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(5):  # 5ä¸ªepoch
        log_with_timestamp(f"Epoch {epoch+1}/5")
        
        epoch_start_time = time.time()
        total_loss = 0
        
        for batch_idx in range(num_batches):
            # åˆ›å»ºéšæœºæ•°æ®ï¼ˆåœ¨GPUä¸Šï¼‰
            inputs = torch.randn(batch_size, input_size, device=device)
            targets = torch.randint(0, 10, (batch_size,), device=device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
            # æ¯10ä¸ªbatchè¾“å‡ºä¸€æ¬¡
            if batch_idx % 10 == 0:
                gpu_memory = torch.cuda.memory_allocated() / 1024**2
                log_with_timestamp(f"  Batch {batch_idx+1}/{num_batches}, Loss: {loss.item():.4f}, GPUå†…å­˜: {gpu_memory:.1f}MB")
            
            # æ·»åŠ ä¸€äº›é¢å¤–çš„GPUè®¡ç®—æ¥å¢åŠ è´Ÿè½½
            if batch_idx % 5 == 0:
                # é¢å¤–çš„çŸ©é˜µè¿ç®—
                extra_computation = torch.matmul(
                    torch.randn(512, 512, device=device),
                    torch.randn(512, 512, device=device)
                )
        
        epoch_time = time.time() - epoch_start_time
        avg_loss = total_loss / num_batches
        log_with_timestamp(f"Epoch {epoch+1} å®Œæˆï¼Œå¹³å‡æŸå¤±: {avg_loss:.4f}, è€—æ—¶: {epoch_time:.2f}ç§’")
        
        # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨æƒ…å†µ
        gpu_memory = torch.cuda.memory_allocated() / 1024**2
        gpu_cached = torch.cuda.memory_reserved() / 1024**2
        log_with_timestamp(f"GPUå†…å­˜ - å·²åˆ†é…: {gpu_memory:.1f}MB, å·²ç¼“å­˜: {gpu_cached:.1f}MB")
    
    log_with_timestamp("è®­ç»ƒå®Œæˆï¼")
    
    # æœ€ç»ˆGPUçŠ¶æ€
    final_memory = torch.cuda.memory_allocated() / 1024**2
    log_with_timestamp(f"æœ€ç»ˆGPUå†…å­˜ä½¿ç”¨: {final_memory:.1f}MB")

if __name__ == "__main__":
    main()
'''
    
    return training_script

def run_gpu_monitoring():
    """è¿è¡ŒGPUç›‘æ§"""
    monitoring_script = '''
import time
import pynvml
from datetime import datetime

def monitor_gpu():
    try:
        pynvml.nvmlInit()
        handle = pynvml.nvmlDeviceGetHandleByIndex(0)
        
        print("å¼€å§‹GPUç›‘æ§...")
        
        for i in range(120):  # ç›‘æ§2åˆ†é’Ÿ
            timestamp = datetime.now().strftime('%H:%M:%S')
            
            # GPUåˆ©ç”¨ç‡
            util = pynvml.nvmlDeviceGetUtilizationRates(handle)
            
            # GPUå†…å­˜
            mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
            
            # GPUæ¸©åº¦
            temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
            
            # GPUåŠŸè€—
            try:
                power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # è½¬æ¢ä¸ºç“¦ç‰¹
            except:
                power = 0
            
            print(f"[{timestamp}] GPU: {util.gpu:3d}% | "
                  f"å†…å­˜: {mem_info.used//1024//1024:4d}MB/{mem_info.total//1024//1024:4d}MB "
                  f"({mem_info.used/mem_info.total*100:5.1f}%) | "
                  f"æ¸©åº¦: {temp:2d}Â°C | "
                  f"åŠŸè€—: {power:5.1f}W")
            
            time.sleep(1)
            
    except Exception as e:
        print(f"GPUç›‘æ§é”™è¯¯: {e}")

if __name__ == "__main__":
    monitor_gpu()
'''
    
    return monitoring_script

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ä¿®å¤ç‰ˆGPUå®éªŒ - è§£å†³GPUåˆ©ç”¨ç‡0%é—®é¢˜")
    print("=" * 60)
    
    # 1. æ£€æŸ¥GPUç¯å¢ƒ
    if not check_and_fix_gpu_environment():
        print("âŒ GPUç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œè¯·å…ˆè§£å†³GPUé—®é¢˜")
        return
    
    # 2. åˆ›å»ºè®­ç»ƒè„šæœ¬
    print("ğŸ“ åˆ›å»ºGPUå¯†é›†å‹è®­ç»ƒè„šæœ¬...")
    training_script = create_intensive_gpu_training_script()
    
    with open("intensive_gpu_training.py", "w") as f:
        f.write(training_script)
    
    # 3. åˆ›å»ºç›‘æ§è„šæœ¬
    print("ğŸ“Š åˆ›å»ºGPUç›‘æ§è„šæœ¬...")
    monitoring_script = run_gpu_monitoring()
    
    with open("gpu_monitor.py", "w") as f:
        f.write(monitoring_script)
    
    print("âœ… è„šæœ¬åˆ›å»ºå®Œæˆ")
    print("\n" + "=" * 60)
    print("ğŸ¯ ä½¿ç”¨è¯´æ˜")
    print("=" * 60)
    print("åœ¨Google Colabä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤:")
    print()
    print("# 1. é¦–å…ˆè¿è¡ŒGPUè¯Šæ–­")
    print("!python gpu_diagnostic.py")
    print()
    print("# 2. åœ¨ä¸€ä¸ªcellä¸­å¯åŠ¨GPUç›‘æ§")
    print("!python gpu_monitor.py &")
    print()
    print("# 3. åœ¨å¦ä¸€ä¸ªcellä¸­è¿è¡ŒGPUè®­ç»ƒ")
    print("!python intensive_gpu_training.py")
    print()
    print("ğŸ’¡ è¿™æ ·æ‚¨åº”è¯¥èƒ½çœ‹åˆ°GPUåˆ©ç”¨ç‡ä¸Šå‡åˆ°50-90%")
    print("=" * 60)

if __name__ == "__main__":
    main()