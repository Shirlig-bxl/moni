#!/usr/bin/env python3
"""
æœ¬åœ°çŽ¯å¢ƒå®Œå…¨ç‹¬ç«‹å®žéªŒè„šæœ¬
ä¸ä¾èµ–å¤æ‚çš„æ¨¡å—å¯¼å…¥ï¼Œç›´æŽ¥è¿è¡Œ
"""

import os
import sys
import time
import threading
import csv
import json
from datetime import datetime
from pathlib import Path
import subprocess


def check_dependencies():
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–"""
    required_packages = ['pandas', 'numpy', 'psutil']
    missing_packages = []
    
    for package in required_packages:
        try:
            __import__(package)
        except ImportError:
            missing_packages.append(package)
    
    if missing_packages:
        print(f"âŒ ç¼ºå°‘ä¾èµ–åŒ…: {missing_packages}")
        print("è¯·è¿è¡Œ: pip3 install " + " ".join(missing_packages))
        return False
    
    return True


class SimpleSystemMonitor:
    """ç®€åŒ–çš„ç³»ç»Ÿç›‘æŽ§å™¨"""
    
    def __init__(self, output_file, interval=1):
        self.output_file = output_file
        self.interval = interval
        self.running = False
        
    def start(self):
        """å¼€å§‹ç›‘æŽ§"""
        import psutil
        
        self.running = True
        
        # åˆ›å»ºCSVæ–‡ä»¶
        with open(self.output_file, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['timestamp', 'cpu_percent', 'memory_percent', 'disk_percent'])
        
        print(f"âœ… ç³»ç»Ÿç›‘æŽ§å·²å¯åŠ¨: {self.output_file}")
        
        while self.running:
            try:
                timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                cpu_percent = psutil.cpu_percent(interval=0.1)
                memory_percent = psutil.virtual_memory().percent
                disk_percent = psutil.disk_usage('/').percent
                
                with open(self.output_file, 'a', newline='') as f:
                    writer = csv.writer(f)
                    writer.writerow([timestamp, cpu_percent, memory_percent, disk_percent])
                
                print(f"[{timestamp}] CPU: {cpu_percent:.1f}%, Memory: {memory_percent:.1f}%, Disk: {disk_percent:.1f}%")
                time.sleep(self.interval)
                
            except Exception as e:
                print(f"ç›‘æŽ§é”™è¯¯: {e}")
                break
    
    def stop(self):
        """åœæ­¢ç›‘æŽ§"""
        self.running = False
        print("âœ… ç³»ç»Ÿç›‘æŽ§å·²åœæ­¢")


class SimpleLogParser:
    """ç®€åŒ–çš„æ—¥å¿—è§£æžå™¨"""
    
    def __init__(self, log_file, output_file):
        self.log_file = log_file
        self.output_file = output_file
        self.running = False
        
    def start(self):
        """å¼€å§‹è§£æž"""
        self.running = True
        
        # åˆ›å»ºCSVæ–‡ä»¶
        with open(self.output_file, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['timestamp', 'loss', 'accuracy', 'learning_rate', 'step', 'epoch', 'event_type'])
        
        print(f"âœ… æ—¥å¿—è§£æžå·²å¯åŠ¨: {self.output_file}")
        
        # ç­‰å¾…æ—¥å¿—æ–‡ä»¶åˆ›å»º
        while self.running and not os.path.exists(self.log_file):
            time.sleep(0.5)
        
        if not self.running:
            return
            
        # è§£æžæ—¥å¿—
        processed_lines = 0
        while self.running:
            try:
                if os.path.exists(self.log_file):
                    with open(self.log_file, 'r') as f:
                        lines = f.readlines()
                    
                    # å¤„ç†æ–°è¡Œ
                    for line in lines[processed_lines:]:
                        self.parse_line(line.strip())
                        processed_lines += 1
                
                time.sleep(1)
                
            except Exception as e:
                print(f"æ—¥å¿—è§£æžé”™è¯¯: {e}")
                break
    
    def parse_line(self, line):
        """è§£æžå•è¡Œæ—¥å¿—"""
        if not line:
            return
            
        try:
            # æå–æ—¶é—´æˆ³
            timestamp_match = line.split(' - ')[0] if ' - ' in line else datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            # æå–æŒ‡æ ‡
            loss = None
            accuracy = None
            learning_rate = None
            step = None
            epoch = None
            event_type = "info"
            
            # è§£æžLoss
            if "Loss:" in line:
                import re
                loss_match = re.search(r'Loss:\s*([0-9]*\.?[0-9]+(?:[eE][-+]?[0-9]+)?|nan)', line)
                if loss_match:
                    loss_val = loss_match.group(1)
                    loss = float('nan') if loss_val == 'nan' else float(loss_val)
            
            # è§£æžAccuracy
            if "Accuracy:" in line:
                import re
                acc_match = re.search(r'Accuracy:\s*([0-9]*\.?[0-9]+)', line)
                if acc_match:
                    accuracy = float(acc_match.group(1))
            
            # è§£æžLearning Rate
            if "LR:" in line:
                import re
                lr_match = re.search(r'LR:\s*([0-9]*\.?[0-9]+(?:[eE][-+]?[0-9]+)?)', line)
                if lr_match:
                    learning_rate = float(lr_match.group(1))
            
            # è§£æžStep
            if "Step" in line:
                import re
                step_match = re.search(r'Step\s*(\d+)', line)
                if step_match:
                    step = int(step_match.group(1))
            
            # è§£æžEpoch
            if "Epoch" in line:
                import re
                epoch_match = re.search(r'Epoch\s*(\d+)', line)
                if epoch_match:
                    epoch = int(epoch_match.group(1))
            
            # æ£€æµ‹äº‹ä»¶ç±»åž‹
            if "ERROR" in line or "CRITICAL" in line:
                event_type = "error"
            elif "WARNING" in line:
                event_type = "warning"
            elif "nan" in line.lower():
                event_type = "anomaly"
            
            # å†™å…¥CSV
            with open(self.output_file, 'a', newline='') as f:
                writer = csv.writer(f)
                writer.writerow([timestamp_match, loss, accuracy, learning_rate, step, epoch, event_type])
                
        except Exception as e:
            print(f"è§£æžè¡Œé”™è¯¯: {e}")
    
    def stop(self):
        """åœæ­¢è§£æž"""
        self.running = False
        print("âœ… æ—¥å¿—è§£æžå·²åœæ­¢")


def generate_training_simulation():
    """ç”Ÿæˆè®­ç»ƒæ¨¡æ‹Ÿæ—¥å¿—"""
    log_file = "local_training.log"
    
    training_logs = [
        "2025-11-05 18:00:00 - INFO - å¼€å§‹BERT-IMDBå¾®è°ƒè®­ç»ƒ",
        "2025-11-05 18:00:05 - INFO - æ¨¡åž‹: bert-base-uncased, è®¾å¤‡: CPU",
        "2025-11-05 18:00:10 - INFO - Epoch 1/3, Step 1/1563, Loss: 0.693, Accuracy: 0.500, LR: 2e-05",
        "2025-11-05 18:00:15 - INFO - Epoch 1/3, Step 50/1563, Loss: 0.620, Accuracy: 0.640, LR: 1.98e-05",
        "2025-11-05 18:00:20 - INFO - Epoch 1/3, Step 100/1563, Loss: 0.580, Accuracy: 0.720, LR: 1.96e-05",
        "2025-11-05 18:00:25 - INFO - Epoch 1/3, Step 150/1563, Loss: 0.540, Accuracy: 0.780, LR: 1.94e-05",
        "2025-11-05 18:00:30 - INFO - Epoch 1/3, Step 200/1563, Loss: 0.500, Accuracy: 0.820, LR: 1.92e-05",
        
        # æ•…éšœæ³¨å…¥ç‚¹1: NaN Loss
        "2025-11-05 18:00:35 - WARNING - æ£€æµ‹åˆ°å­¦ä¹ çŽ‡å¼‚å¸¸ï¼ŒLosså¼€å§‹ä¸ç¨³å®š",
        "2025-11-05 18:00:36 - ERROR - Loss: nan, Accuracy: 0.000, LR: 1.90e-05",
        "2025-11-05 18:00:37 - CRITICAL - NaN detected in loss computation",
        "2025-11-05 18:00:40 - INFO - è®­ç»ƒå·²æ¢å¤, Loss: 0.520, Accuracy: 0.800, LR: 1.88e-05",
        
        "2025-11-05 18:00:45 - INFO - Epoch 1/3, Step 300/1563, Loss: 0.480, Accuracy: 0.840, LR: 1.86e-05",
        "2025-11-05 18:00:50 - INFO - Epoch 1/3, Step 400/1563, Loss: 0.450, Accuracy: 0.860, LR: 1.84e-05",
        
        # æ•…éšœæ³¨å…¥ç‚¹2: I/Oç“¶é¢ˆ
        "2025-11-05 18:01:00 - WARNING - æ•°æ®åŠ è½½é€Ÿåº¦ä¸‹é™ï¼Œæ£€æµ‹åˆ°I/Oç“¶é¢ˆ",
        "2025-11-05 18:01:05 - INFO - æ•°æ®åŠ è½½å»¶è¿Ÿ: 5.2s (æ­£å¸¸: 0.1s)",
        "2025-11-05 18:01:10 - INFO - I/Oç“¶é¢ˆå·²è§£å†³ï¼Œè®­ç»ƒç»§ç»­",
        
        "2025-11-05 18:01:15 - INFO - Epoch 1/3, Step 500/1563, Loss: 0.420, Accuracy: 0.880, LR: 1.82e-05",
        "2025-11-05 18:01:20 - INFO - Epoch 2/3, Step 1563/3126, Loss: 0.300, Accuracy: 0.940, LR: 1.76e-05",
        
        # æ•…éšœæ³¨å…¥ç‚¹3: èµ„æºäº‰ç”¨
        "2025-11-05 18:01:50 - WARNING - æ£€æµ‹åˆ°CPUèµ„æºäº‰ç”¨",
        "2025-11-05 18:01:55 - INFO - CPUåˆ©ç”¨çŽ‡å¼‚å¸¸: 95% (æ­£å¸¸: 60%)",
        "2025-11-05 18:02:00 - INFO - èµ„æºäº‰ç”¨å·²è§£å†³",
        
        "2025-11-05 18:02:05 - INFO - Epoch 3/3, Step 3126/3126, Loss: 0.240, Accuracy: 0.970, LR: 1.70e-05",
        "2025-11-05 18:02:10 - INFO - è®­ç»ƒå®Œæˆï¼æœ€ç»ˆéªŒè¯å‡†ç¡®çŽ‡: 0.975",
    ]
    
    print("ðŸš€ å¼€å§‹ç”Ÿæˆè®­ç»ƒæ¨¡æ‹Ÿæ—¥å¿—...")
    
    with open(log_file, 'w') as f:
        for i, log in enumerate(training_logs):
            f.write(log + '\n')
            f.flush()
            
            # æ˜¾ç¤ºè¿›åº¦
            if i % 3 == 0:
                progress = (i + 1) / len(training_logs) * 100
                print(f"è®­ç»ƒè¿›åº¦: {progress:.1f}% - {log.split(' - ')[-1]}")
            
            time.sleep(1)  # æ¨¡æ‹Ÿè®­ç»ƒæ—¶é—´
    
    print(f"âœ… è®­ç»ƒæ¨¡æ‹Ÿå®Œæˆ: {log_file}")
    return log_file


def aggregate_data_simple():
    """ç®€å•çš„æ•°æ®èšåˆ"""
    print("\nðŸ“Š å¼€å§‹æ•°æ®èšåˆ...")
    
    try:
        import pandas as pd
        
        # è¯»å–æ•°æ®æ–‡ä»¶
        system_df = pd.read_csv("local_system_metrics.csv")
        training_df = pd.read_csv("local_training_metrics.csv")
        
        # è½¬æ¢æ—¶é—´æˆ³
        system_df['timestamp'] = pd.to_datetime(system_df['timestamp'])
        training_df['timestamp'] = pd.to_datetime(training_df['timestamp'])
        
        # æŒ‰æ—¶é—´æˆ³åˆå¹¶
        merged_df = pd.merge(system_df, training_df, on='timestamp', how='outer')
        merged_df = merged_df.sort_values('timestamp')
        
        # å¡«å……ç¼ºå¤±å€¼
        merged_df = merged_df.fillna(method='ffill').fillna(method='bfill')
        
        # æ·»åŠ å¼‚å¸¸æ ‡æ³¨
        merged_df['is_anomaly'] = 0
        
        # æ ‡è®°å¼‚å¸¸æ—¶é—´æ®µ
        anomaly_periods = [
            ('2025-11-05 18:00:35', '2025-11-05 18:00:40'),  # NaN Loss
            ('2025-11-05 18:01:00', '2025-11-05 18:01:10'),  # I/Oç“¶é¢ˆ
            ('2025-11-05 18:01:50', '2025-11-05 18:02:00'),  # èµ„æºäº‰ç”¨
        ]
        
        for start_time, end_time in anomaly_periods:
            mask = (merged_df['timestamp'] >= start_time) & (merged_df['timestamp'] <= end_time)
            merged_df.loc[mask, 'is_anomaly'] = 1
        
        # ä¿å­˜èšåˆæ•°æ®
        output_file = "local_aggregated_data.csv"
        merged_df.to_csv(output_file, index=False)
        
        anomaly_count = merged_df['is_anomaly'].sum()
        print(f"âœ… æ•°æ®èšåˆå®Œæˆ: {len(merged_df)} æ¡è®°å½•, {anomaly_count} ä¸ªå¼‚å¸¸ç‚¹")
        print(f"ðŸ’¾ èšåˆæ•°æ®å·²ä¿å­˜: {output_file}")
        
        return merged_df
        
    except Exception as e:
        print(f"âŒ æ•°æ®èšåˆå¤±è´¥: {e}")
        return None


def main():
    """ä¸»å‡½æ•°"""
    print("ðŸš€ æœ¬åœ°çŽ¯å¢ƒå®Œå…¨ç‹¬ç«‹å®žéªŒ")
    print("=" * 50)
    
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        return
    
    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs("local_results", exist_ok=True)
        
        # 1. å¯åŠ¨ç›‘æŽ§
        print("\nðŸ“Š å¯åŠ¨ç³»ç»Ÿç›‘æŽ§...")
        system_monitor = SimpleSystemMonitor("local_system_metrics.csv", interval=1)
        monitor_thread = threading.Thread(target=system_monitor.start)
        monitor_thread.daemon = True
        monitor_thread.start()
        
        # 2. å¯åŠ¨æ—¥å¿—è§£æž
        log_parser = SimpleLogParser("local_training.log", "local_training_metrics.csv")
        parser_thread = threading.Thread(target=log_parser.start)
        parser_thread.daemon = True
        parser_thread.start()
        
        time.sleep(2)  # ç­‰å¾…ç›‘æŽ§å¯åŠ¨
        
        # 3. ç”Ÿæˆè®­ç»ƒæ¨¡æ‹Ÿ
        print("\nðŸš€ å¼€å§‹è®­ç»ƒæ¨¡æ‹Ÿ...")
        log_file = generate_training_simulation()
        
        # 4. ç­‰å¾…æ•°æ®æ”¶é›†
        print("\nâ³ ç­‰å¾…æ•°æ®æ”¶é›†å®Œæˆ...")
        time.sleep(5)
        
        # 5. åœæ­¢ç›‘æŽ§
        system_monitor.stop()
        log_parser.stop()
        
        time.sleep(2)  # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        
        # 6. æ•°æ®èšåˆ
        aggregated_data = aggregate_data_simple()
        
        # 7. å®žéªŒæ€»ç»“
        print("\nðŸŽ‰ æœ¬åœ°å®žéªŒå®Œæˆï¼")
        print("\nðŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        
        files = [
            "local_training.log",
            "local_system_metrics.csv",
            "local_training_metrics.csv",
            "local_aggregated_data.csv"
        ]
        
        total_size = 0
        for file in files:
            if os.path.exists(file):
                size = os.path.getsize(file)
                total_size += size
                print(f"  âœ… {file} ({size:,} bytes)")
            else:
                print(f"  âŒ {file} (æœªç”Ÿæˆ)")
        
        print(f"\nðŸ“Š æ€»æ•°æ®é‡: {total_size:,} bytes ({total_size/1024/1024:.2f} MB)")
        
        if aggregated_data is not None:
            print(f"\nðŸ“ˆ æ•°æ®è´¨é‡æŠ¥å‘Š:")
            print(f"  - æ—¶é—´èŒƒå›´: {len(aggregated_data)} ä¸ªæ—¶é—´ç‚¹")
            print(f"  - ç‰¹å¾æ•°é‡: {len(aggregated_data.columns)} ä¸ª")
            anomaly_ratio = aggregated_data['is_anomaly'].sum() / len(aggregated_data) * 100
            print(f"  - å¼‚å¸¸æ¯”ä¾‹: {anomaly_ratio:.1f}%")
        
        print("\nðŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
        print("1. ä½¿ç”¨Google Colabè¿è¡ŒGPUç‰ˆæœ¬èŽ·å¾—æ›´å¥½æ•ˆæžœ")
        print("2. åˆ†æžç”Ÿæˆçš„local_aggregated_data.csvè¿›è¡Œå¼‚å¸¸æ£€æµ‹ç ”ç©¶")
        print("3. åŸºäºŽå®žéªŒæ•°æ®å®Œå–„æ‚¨çš„TSE-Matrixå’ŒMLH-ADæ¡†æž¶")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ å®žéªŒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å®žéªŒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()